# rag_setup.py
import os
import json
from datetime import datetime

from dotenv import load_dotenv
from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

load_dotenv(override=True)

vector_store = None
TIMESTAMP_FILE = "rag_timestamp.json"


def get_file_mtime(file_path):
    """ç²å–æ–‡ä»¶çš„ä¿®æ”¹æ™‚é–“"""
    if os.path.exists(file_path):
        return os.path.getmtime(file_path)
    return 0


def load_timestamp():
    """è¼‰å…¥ä¸Šæ¬¡ embedding çš„æ™‚é–“æˆ³è¨˜"""
    if os.path.exists(TIMESTAMP_FILE):
        try:
            with open(TIMESTAMP_FILE, "r", encoding="utf-8") as f:
                return json.load(f).get("last_embedding_time", 0)
        except:
            return 0
    return 0


def save_timestamp():
    """ä¿å­˜ç•¶å‰æ™‚é–“æˆ³è¨˜"""
    with open(TIMESTAMP_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_embedding_time": datetime.now().timestamp()}, f)


def needs_re_embedding(markdown_path):
    """æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–° embedding"""
    if not vector_store:
        return True
    
    file_mtime = get_file_mtime(markdown_path)
    last_embedding_time = load_timestamp()
    
    return file_mtime > last_embedding_time


def setup_rag():
    global vector_store

    # çŸ¥è­˜åº«æ–‡ä»¶è·¯å¾‘ - å¾žç’°å¢ƒè®Šæ•¸è®€å–ï¼Œé è¨­ç‚ºç›¸å°è·¯å¾‘
    markdown_path = os.getenv("KNOWLEDGE_BASE_PATH", "docs/Museum_Collection_Info.md")

    if not os.path.exists(markdown_path):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°çŸ¥è­˜åº«æ–‡ä»¶ {markdown_path}")
        print(f"è«‹ç¢ºä¿æª”æ¡ˆè·¯å¾‘æ­£ç¢ºï¼Œæˆ–è¨­ç½® KNOWLEDGE_BASE_PATH ç’°å¢ƒè®Šæ•¸")
        # å‰µå»ºä¸€å€‹ç©ºçš„å‘é‡å­˜å„²ä»¥é¿å…ç¨‹å¼å´©æ½°
        vector_store = None
        return
    
    # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–° embedding
    if not needs_re_embedding(markdown_path):
        print("ðŸ“„ çŸ¥è­˜åº«æ–‡ä»¶æœªæ›´æ–°ï¼Œè·³éŽ embedding ç¨‹åº")
        return

    try:
        with open(markdown_path, "r", encoding="utf-8") as f:
            markdown_document = f.read()

        if not markdown_document.strip():
            print(f"è­¦å‘Šï¼šçŸ¥è­˜åº«æ–‡ä»¶ {markdown_path} æ˜¯ç©ºçš„")
            vector_store = None
            return

        # 1. å®šç¾©è¦æ ¹æ“šå“ªäº› Markdown æ¨™é¡Œé€²è¡Œåˆ†å‰²
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]

        # 2. å¯¦ä¾‹åŒ– Markdown åˆ†å‰²å™¨
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        md_header_splits = markdown_splitter.split_text(markdown_document)

        if not md_header_splits:
            print(f"è­¦å‘Šï¼šç„¡æ³•å¾ž {markdown_path} ä¸­æå–åˆ°ä»»ä½•æ–‡æª”ç‰‡æ®µ")
            vector_store = None
            return

        # 3. å»ºç«‹ embeddings å’Œå‘é‡å„²å­˜
        # LangChain çš„ FAISS.from_documents æœƒè‡ªå‹•è™•ç† Document ç‰©ä»¶
        print("ðŸ”„ æ­£åœ¨é‡æ–°å»ºç«‹å‘é‡å„²å­˜...")
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vector_store = FAISS.from_documents(md_header_splits, embeddings)
        
        # ä¿å­˜æ™‚é–“æˆ³è¨˜
        save_timestamp()
        print("âœ… RAG ç³»çµ±å·²æˆåŠŸåˆå§‹åŒ– (ä½¿ç”¨ Markdown åˆ†å‰²)ï¼")
        
    except FileNotFoundError:
        print(f"éŒ¯èª¤ï¼šç„¡æ³•è®€å–çŸ¥è­˜åº«æ–‡ä»¶ {markdown_path}")
        vector_store = None
    except UnicodeDecodeError:
        print(f"éŒ¯èª¤ï¼šçŸ¥è­˜åº«æ–‡ä»¶ {markdown_path} ç·¨ç¢¼æ ¼å¼ä¸æ­£ç¢º")
        vector_store = None
    except Exception as e:
        print(f"éŒ¯èª¤ï¼šåˆå§‹åŒ– RAG ç³»çµ±æ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
        vector_store = None


# search_knowledge_base å‡½æ•¸ä¿æŒä¸è®Š
def search_knowledge_base(query: str) -> str:
    global vector_store
    if not vector_store:
        return "çŸ¥è­˜åº«å°šæœªåˆå§‹åŒ–ã€‚"

    retriever = vector_store.as_retriever(search_kwargs={"k": 3})  # å¢žåŠ æª¢ç´¢æ•¸é‡ä»¥æ‡‰å°æ¯”è¼ƒå•é¡Œ
    relevant_docs = retriever.invoke(query)

    context = "\n\n---\n\n".join(
        [
            f"ä¾†æº: {doc.metadata.get('Header 2', '')} > {doc.metadata.get('Header 3', '')}\nå…§å®¹: {doc.page_content}"
            for doc in relevant_docs
        ]
    )
    return f"å¾žçŸ¥è­˜åº«ä¸­æ‰¾åˆ°çš„ç›¸é—œè³‡è¨Šå¦‚ä¸‹ï¼š\n{context}"
