# rag_setup.py
import os
import json
from datetime import datetime

from dotenv import load_dotenv
from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

load_dotenv(override=True)

vector_store = None
TIMESTAMP_FILE = "rag_timestamp.json"
FAISS_INDEX_DIR = "faiss_index"
FAISS_INDEX_NAME = "knowledge_base"


def get_file_mtime(file_path):
    """ç²å–æ–‡ä»¶çš„ä¿®æ”¹æ™‚é–“"""
    if os.path.exists(file_path):
        return os.path.getmtime(file_path)
    return 0


def load_timestamp():
    """è¼‰å…¥ä¸Šæ¬¡ embedding çš„æ™‚é–“æˆ³è¨˜"""
    if os.path.exists(TIMESTAMP_FILE):
        try:
            with open(TIMESTAMP_FILE, "r", encoding="utf-8") as f:
                return json.load(f).get("last_embedding_time", 0)
        except:
            return 0
    return 0


def save_timestamp():
    """ä¿å­˜ç•¶å‰æ™‚é–“æˆ³è¨˜"""
    with open(TIMESTAMP_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_embedding_time": datetime.now().timestamp()}, f)


def save_local():
    """å°‡ FAISS å‘é‡å„²å­˜ä¿å­˜åˆ°æœ¬åœ°"""
    global vector_store
    if not vector_store:
        print("âŒ ç„¡æ³•ä¿å­˜ï¼šå‘é‡å„²å­˜å°šæœªåˆå§‹åŒ–")
        return False
    
    try:
        os.makedirs(FAISS_INDEX_DIR, exist_ok=True)
        vector_store.save_local(FAISS_INDEX_DIR, FAISS_INDEX_NAME)
        save_timestamp()
        print(f"ðŸ’¾ FAISS å‘é‡å„²å­˜å·²ä¿å­˜è‡³ {FAISS_INDEX_DIR}/{FAISS_INDEX_NAME}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜ FAISS å‘é‡å„²å­˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False


def load_local():
    """å¾žæœ¬åœ°è¼‰å…¥ FAISS å‘é‡å„²å­˜"""
    global vector_store
    
    faiss_path = os.path.join(FAISS_INDEX_DIR, f"{FAISS_INDEX_NAME}.faiss")
    pkl_path = os.path.join(FAISS_INDEX_DIR, f"{FAISS_INDEX_NAME}.pkl")
    
    if not (os.path.exists(faiss_path) and os.path.exists(pkl_path)):
        print(f"ðŸ“ æ‰¾ä¸åˆ°ç¾æœ‰çš„ FAISS ç´¢å¼•æª”æ¡ˆ")
        return False
    
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vector_store = FAISS.load_local(
            FAISS_INDEX_DIR, 
            embeddings, 
            FAISS_INDEX_NAME,
            allow_dangerous_deserialization=True
        )
        print(f"ðŸ“‚ å·²æˆåŠŸè¼‰å…¥ FAISS å‘é‡å„²å­˜")
        return True
    except Exception as e:
        print(f"âŒ è¼‰å…¥ FAISS å‘é‡å„²å­˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        vector_store = None
        return False


def needs_re_embedding(markdown_path):
    """æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–° embedding"""
    if not vector_store:
        return True
    
    file_mtime = get_file_mtime(markdown_path)
    last_embedding_time = load_timestamp()
    
    return file_mtime > last_embedding_time


def setup_rag():
    global vector_store

    # çŸ¥è­˜åº«æ–‡ä»¶è·¯å¾‘ - å¾žç’°å¢ƒè®Šæ•¸è®€å–ï¼Œé è¨­ç‚ºç›¸å°è·¯å¾‘
    markdown_path = os.getenv("KNOWLEDGE_BASE_PATH", "docs/Museum_Collection_Info.md")

    if not os.path.exists(markdown_path):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°çŸ¥è­˜åº«æ–‡ä»¶ {markdown_path}")
        print(f"è«‹ç¢ºä¿æª”æ¡ˆè·¯å¾‘æ­£ç¢ºï¼Œæˆ–è¨­ç½® KNOWLEDGE_BASE_PATH ç’°å¢ƒè®Šæ•¸")
        # å‰µå»ºä¸€å€‹ç©ºçš„å‘é‡å­˜å„²ä»¥é¿å…ç¨‹å¼å´©æ½°
        vector_store = None
        return
    
    # é¦–å…ˆå˜—è©¦è¼‰å…¥ç¾æœ‰çš„ FAISS ç´¢å¼•
    if load_local():
        # æª¢æŸ¥è¼‰å…¥çš„å‘é‡å„²å­˜æ˜¯å¦éœ€è¦æ›´æ–°
        if not needs_re_embedding(markdown_path):
            print("ðŸ“„ å·²è¼‰å…¥ç¾æœ‰å‘é‡å„²å­˜ï¼ŒçŸ¥è­˜åº«æ–‡ä»¶æœªæ›´æ–°")
            return
        else:
            print("ðŸ“„ çŸ¥è­˜åº«æ–‡ä»¶å·²æ›´æ–°ï¼Œéœ€è¦é‡æ–°å»ºç«‹å‘é‡å„²å­˜")
    else:
        print("ðŸ“„ æœªæ‰¾åˆ°ç¾æœ‰å‘é‡å„²å­˜ï¼Œå°‡å»ºç«‹æ–°çš„ç´¢å¼•")

    try:
        with open(markdown_path, "r", encoding="utf-8") as f:
            markdown_document = f.read()

        if not markdown_document.strip():
            print(f"è­¦å‘Šï¼šçŸ¥è­˜åº«æ–‡ä»¶ {markdown_path} æ˜¯ç©ºçš„")
            vector_store = None
            return

        # 1. å®šç¾©è¦æ ¹æ“šå“ªäº› Markdown æ¨™é¡Œé€²è¡Œåˆ†å‰²
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]

        # 2. å¯¦ä¾‹åŒ– Markdown åˆ†å‰²å™¨
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        md_header_splits = markdown_splitter.split_text(markdown_document)

        if not md_header_splits:
            print(f"è­¦å‘Šï¼šç„¡æ³•å¾ž {markdown_path} ä¸­æå–åˆ°ä»»ä½•æ–‡æª”ç‰‡æ®µ")
            vector_store = None
            return

        # 3. å»ºç«‹ embeddings å’Œå‘é‡å„²å­˜
        # LangChain çš„ FAISS.from_documents æœƒè‡ªå‹•è™•ç† Document ç‰©ä»¶
        print("ðŸ”„ æ­£åœ¨é‡æ–°å»ºç«‹å‘é‡å„²å­˜...")
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vector_store = FAISS.from_documents(md_header_splits, embeddings)
        
        # ä¿å­˜å‘é‡å„²å­˜åˆ°æœ¬åœ°
        if save_local():
            print("âœ… RAG ç³»çµ±å·²æˆåŠŸåˆå§‹åŒ–ä¸¦ä¿å­˜ (ä½¿ç”¨ Markdown åˆ†å‰²)ï¼")
        else:
            print("âš ï¸  RAG ç³»çµ±å·²åˆå§‹åŒ–ï¼Œä½†ä¿å­˜å¤±æ•—")
        
    except FileNotFoundError:
        print(f"éŒ¯èª¤ï¼šç„¡æ³•è®€å–çŸ¥è­˜åº«æ–‡ä»¶ {markdown_path}")
        vector_store = None
    except UnicodeDecodeError:
        print(f"éŒ¯èª¤ï¼šçŸ¥è­˜åº«æ–‡ä»¶ {markdown_path} ç·¨ç¢¼æ ¼å¼ä¸æ­£ç¢º")
        vector_store = None
    except Exception as e:
        print(f"éŒ¯èª¤ï¼šåˆå§‹åŒ– RAG ç³»çµ±æ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
        vector_store = None


# search_knowledge_base å‡½æ•¸ä¿æŒä¸è®Š
def search_knowledge_base(query: str) -> str:
    global vector_store
    if not vector_store:
        return "çŸ¥è­˜åº«å°šæœªåˆå§‹åŒ–ã€‚"

    retriever = vector_store.as_retriever(search_kwargs={"k": 3})  # å¢žåŠ æª¢ç´¢æ•¸é‡ä»¥æ‡‰å°æ¯”è¼ƒå•é¡Œ
    relevant_docs = retriever.invoke(query)

    context = "\n\n---\n\n".join(
        [
            f"ä¾†æº: {doc.metadata.get('Header 2', '')} > {doc.metadata.get('Header 3', '')}\nå…§å®¹: {doc.page_content}"
            for doc in relevant_docs
        ]
    )
    return f"å¾žçŸ¥è­˜åº«ä¸­æ‰¾åˆ°çš„ç›¸é—œè³‡è¨Šå¦‚ä¸‹ï¼š\n{context}"
